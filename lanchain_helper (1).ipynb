{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Langchain RAG Model \n",
    "\n",
    "## ğŸ¯ Our Mission: Building a Smart Q&A System\n",
    "\n",
    "Welcome, AI explorer! In this notebook, we're embarking on an exciting journey to create a Retrieval-Augmented Generation (RAG) model using Langchain. But what exactly are we building, and why? Let's break it down:\n",
    "\n",
    "### ğŸ§  What is RAG?\n",
    "\n",
    "RAG stands for Retrieval-Augmented Generation. It's a powerful approach that combines:\n",
    "- ğŸ“š The vast knowledge of large language models\n",
    "- ğŸ” The precision of information retrieval systems\n",
    "\n",
    "### ğŸ› ï¸ What We're Building\n",
    "\n",
    "We're creating a smart question-answering system that can:\n",
    "1. ğŸ“¥ Ingest and process web content\n",
    "2. ğŸ—„ï¸ Store information efficiently in a vector database\n",
    "3. ğŸ” Retrieve relevant information for any given query\n",
    "4. ğŸ¤– Generate accurate, context-aware responses\n",
    "\n",
    "### ğŸŒŸ Why It Matters\n",
    "\n",
    "This RAG model will be able to:\n",
    "- ğŸ’¡ Answer questions with up-to-date, relevant information\n",
    "- ğŸ¨ Provide creative, contextual responses (like related poems!)\n",
    "- ğŸ”„ Suggest intelligent follow-up questions\n",
    "\n",
    "### ğŸš€ Let's Get Started!\n",
    "\n",
    "Throughout this notebook, we'll walk through each step of building our RAG model. By the end, you'll have a powerful, flexible system for answering questions with the added context of a curated knowledge base.\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š Importing Libraries for Document Processing\n",
    "\n",
    "In this cell, we're importing essential libraries for loading and processing web documents:\n",
    "\n",
    "- ğŸŒ `WebBaseLoader`: Our gateway to fetching web content\n",
    "- ğŸ² `bs4` (BeautifulSoup): Our trusted web scraping companion\n",
    "- âœ‚ï¸ `RecursiveCharacterTextSplitter`: Our text chunking expert\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader \n",
    "import bs4 # beautifulsoup web scraper library\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§  Importing Embedding and Vector Database Libraries\n",
    "\n",
    "Here, we're bringing in the tools for creating and managing our knowledge base:\n",
    "\n",
    "- ğŸ”¢ `OpenAIEmbeddings`: Transforming text into numerical representations\n",
    "- ğŸ—„ï¸ `FAISS`: Our high-performance vector database\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS # vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– Importing Language Model Libraries\n",
    "\n",
    "In this cell, we're importing the big guns - our language model interfaces:\n",
    "\n",
    "- ğŸ–‹ï¸ `ChatOpenAI`: Our bridge to OpenAI's powerful chat models\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ­ Importing Prompt Template Library\n",
    "\n",
    "Here, we're bringing in the tools to craft the perfect prompts:\n",
    "\n",
    "- ğŸ“ `ChatPromptTemplate`: Our secret sauce for guiding AI responses\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Importing I/O and Runnable Libraries\n",
    "\n",
    "Here, we're importing utilities for smooth input/output operations:\n",
    "\n",
    "- ğŸ”„ `RunnablePassthrough`: Our input processing wizard\n",
    "- ğŸ“Š `StrOutputParser`: Ensuring our outputs are string-perfect\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Importing Response Formatting Libraries\n",
    "\n",
    "In this cell, we're bringing in tools to structure our AI's responses:\n",
    "\n",
    "- ğŸ—ï¸ `BaseModel`, `Field`: Building blocks for well-organized outputs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing essential libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– Initializing Our Language Model\n",
    "\n",
    "In this cell, we're firing up our main language model:\n",
    "\n",
    "- ğŸ”§ Model: gpt-3.5-turbo\n",
    "- ğŸ—£ï¸ Verbose mode: On\n",
    "- ğŸŒ¡ï¸ Temperature: 0.5 (balanced creativity)\n",
    "- ğŸ”‘ API Key: Securely loaded\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt= ChatOpenAI(model='gpt-3.5-turbo',\n",
    "                   verbose=True,\n",
    "                   temperature=0.5,\n",
    "                   openai_api_key=os.getenv('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸŒ Defining Our Knowledge Sources\n",
    "\n",
    "In this cell, we're specifying the URLs for our RAG model's knowledge base:\n",
    "\n",
    "- ğŸ“š AI Agents Blog Post\n",
    "- ğŸ“˜ Langchain RAG Tutorial\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=[\"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "      \"https://python.langchain.com/v0.2/docs/tutorials/rag/\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¥ Fetching and Processing Web Content\n",
    "\n",
    "Here, we're loading and preprocessing our web data:\n",
    "\n",
    "- ğŸ•¸ï¸ Using WebBaseLoader to fetch content\n",
    "- ğŸ² Employing BeautifulSoup for parsing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = WebBaseLoader(web_paths=(urls),bs_kwargs=dict(parse_only=bs4.SoupStrainer()))\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ‚ï¸ Chunking Our Text Data\n",
    "\n",
    "In this cell, we're breaking down our text into manageable pieces:\n",
    "\n",
    "- ğŸ“ Chunk size: 1000 characters\n",
    "- ğŸ”€ Overlap: 200 characters\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—„ï¸ Creating Our Vector Database\n",
    "\n",
    "Here, we're setting up our FAISS vector database:\n",
    "\n",
    "- ğŸ”¢ Creating embeddings with OpenAI\n",
    "- ğŸ“š Storing document chunks for quick retrieval\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore =  FAISS.from_documents(splits, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Setting Up Our Information Retriever\n",
    "\n",
    "In this cell, we're creating a retriever from our vector database:\n",
    "\n",
    "- ğŸ•µï¸ Ready to fetch relevant information for user queries\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Crafting Our System Prompt\n",
    "\n",
    "Here, we're defining the behavior of our AI assistant:\n",
    "\n",
    "- ğŸ­ Role: Question-answering assistant\n",
    "- ğŸ“š Context: Use retrieved information\n",
    "- ğŸš« Honesty: Admit when answer is unknown\n",
    "- ğŸ“ Conciseness: Max three sentences\n",
    "- ğŸ”§ Output: JSON format\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    "    \"Your response should be a valid JSON object.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§© Building Our Prompt Template\n",
    "\n",
    "In this cell, we're creating a structured prompt template:\n",
    "\n",
    "- ğŸ¤– System message: Defining AI behavior\n",
    "- ğŸ‘¤ Human input: Placeholder for user queries\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =ChatPromptTemplate.from_messages(\n",
    "   [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ],       \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Defining Our Response Structure\n",
    "\n",
    "Here, we're creating a format for our AI's responses:\n",
    "\n",
    "- âœ… Answer: The main response to the query\n",
    "- ğŸµ Poem: A related poetic touch\n",
    "- â“ Follow-up: Suggested next question\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseFormatter(BaseModel):\n",
    "    answer: str = Field(description=\"The answer to the user's question\")\n",
    "    poem: str = Field(description='poem related to the user query')\n",
    "    followup_question: str = Field(description=\"A followup question the user could ask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ Enabling Structured Outputs\n",
    "\n",
    "In this cell, we're ensuring our LLM provides structured responses:\n",
    "\n",
    "- ğŸ”§ Binding our ResponseFormatter to the LLM\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=chatgpt.with_structured_output(ResponseFormatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“„ Creating Document Formatting Function\n",
    "\n",
    "Here, we're defining a helper function to format retrieved documents:\n",
    "\n",
    "- ğŸ”— Joining document contents with newlines\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ Defining JSON Conversion Function\n",
    "\n",
    "In this cell, we're creating a function to convert responses to JSON:\n",
    "\n",
    "- ğŸ“¦ Transforming structured output to JSON string\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_json_string(response_formatter: ResponseFormatter) -> str:\n",
    "    return json.dumps(response_formatter.dict(), indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”— Assembling Our RAG Chain\n",
    "\n",
    "Here, we're putting all the pieces together:\n",
    "\n",
    "1. ğŸ” Retrieve relevant documents\n",
    "2. ğŸ“ Apply our prompt template\n",
    "3. ğŸ¤– Process through our LLM\n",
    "4. ğŸ”„ Convert to JSON string\n",
    "5. ğŸ“Š Parse the output\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs, \n",
    "        \"input\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | to_json_string\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ Testing Our RAG Model\n",
    "\n",
    "In this cell, we're putting our model to the test:\n",
    "\n",
    "- â“ Query: \"What is an AGENT?\"\n",
    "- ğŸ§  Generating a comprehensive response\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer:\n",
      "An agent is a virtual character controlled by a Large Language Model (LLM) in the context of Generative Agents Simulation. These agents exhibit human-like behavior and interact with each other in a sandbox environment.\n",
      "\n",
      "\n",
      "poem:\n",
      "An agent in the virtual space, controlled with grace, interacts in a simulated place.\n",
      "\n",
      "\n",
      "followup_question:\n",
      "Would you like to know more about how agents are designed and function in autonomous systems?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query='What is an AGENT?'\n",
    "response=rag_chain.invoke(query)\n",
    "json_response=json.loads(response)\n",
    "\n",
    "for key, value in json_response.items():\n",
    "    print(f'{key}:')\n",
    "    print(value)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ‰ Congratulations! You've Built a RAG Model! ğŸš€\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒŸ What We've Accomplished\n",
    "\n",
    "In this notebook, we've embarked on an exciting journey through the world of Retrieval-Augmented Generation (RAG). Let's recap our amazing achievements:\n",
    "\n",
    "1. ğŸ“š Imported and organized a powerful set of libraries\n",
    "2. ğŸ§  Initialized a state-of-the-art language model\n",
    "3. ğŸŒ Loaded and processed web data to create our knowledge base\n",
    "4. ğŸ—„ï¸ Built a vector database for efficient information retrieval\n",
    "5. ğŸ¨ Crafted an intelligent prompt system\n",
    "6. ğŸ”— Assembled a comprehensive RAG chain\n",
    "7. ğŸš€ Tested our model with real-world queries\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "###  Happy coding, and may your AI adventures be ever fruitful! ğŸŒ±"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
