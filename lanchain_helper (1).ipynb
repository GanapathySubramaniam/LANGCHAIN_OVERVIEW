{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 Langchain RAG Model \n",
    "\n",
    "## 🎯 Our Mission: Building a Smart Q&A System\n",
    "\n",
    "Welcome, AI explorer! In this notebook, we're embarking on an exciting journey to create a Retrieval-Augmented Generation (RAG) model using Langchain. But what exactly are we building, and why? Let's break it down:\n",
    "\n",
    "### 🧠 What is RAG?\n",
    "\n",
    "RAG stands for Retrieval-Augmented Generation. It's a powerful approach that combines:\n",
    "- 📚 The vast knowledge of large language models\n",
    "- 🔍 The precision of information retrieval systems\n",
    "\n",
    "### 🛠️ What We're Building\n",
    "\n",
    "We're creating a smart question-answering system that can:\n",
    "1. 📥 Ingest and process web content\n",
    "2. 🗄️ Store information efficiently in a vector database\n",
    "3. 🔎 Retrieve relevant information for any given query\n",
    "4. 🤖 Generate accurate, context-aware responses\n",
    "\n",
    "### 🌟 Why It Matters\n",
    "\n",
    "This RAG model will be able to:\n",
    "- 💡 Answer questions with up-to-date, relevant information\n",
    "- 🎨 Provide creative, contextual responses (like related poems!)\n",
    "- 🔄 Suggest intelligent follow-up questions\n",
    "\n",
    "### 🚀 Let's Get Started!\n",
    "\n",
    "Throughout this notebook, we'll walk through each step of building our RAG model. By the end, you'll have a powerful, flexible system for answering questions with the added context of a curated knowledge base.\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 Importing Libraries for Document Processing\n",
    "\n",
    "In this cell, we're importing essential libraries for loading and processing web documents:\n",
    "\n",
    "- 🌐 `WebBaseLoader`: Our gateway to fetching web content\n",
    "- 🍲 `bs4` (BeautifulSoup): Our trusted web scraping companion\n",
    "- ✂️ `RecursiveCharacterTextSplitter`: Our text chunking expert\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader \n",
    "import bs4 # beautifulsoup web scraper library\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 Importing Embedding and Vector Database Libraries\n",
    "\n",
    "Here, we're bringing in the tools for creating and managing our knowledge base:\n",
    "\n",
    "- 🔢 `OpenAIEmbeddings`: Transforming text into numerical representations\n",
    "- 🗄️ `FAISS`: Our high-performance vector database\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS # vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 Importing Language Model Libraries\n",
    "\n",
    "In this cell, we're importing the big guns - our language model interfaces:\n",
    "\n",
    "- 🖋️ `ChatOpenAI`: Our bridge to OpenAI's powerful chat models\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎭 Importing Prompt Template Library\n",
    "\n",
    "Here, we're bringing in the tools to craft the perfect prompts:\n",
    "\n",
    "- 📝 `ChatPromptTemplate`: Our secret sauce for guiding AI responses\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Importing I/O and Runnable Libraries\n",
    "\n",
    "Here, we're importing utilities for smooth input/output operations:\n",
    "\n",
    "- 🔄 `RunnablePassthrough`: Our input processing wizard\n",
    "- 📊 `StrOutputParser`: Ensuring our outputs are string-perfect\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 Importing Response Formatting Libraries\n",
    "\n",
    "In this cell, we're bringing in tools to structure our AI's responses:\n",
    "\n",
    "- 🏗️ `BaseModel`, `Field`: Building blocks for well-organized outputs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing essential libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 Initializing Our Language Model\n",
    "\n",
    "In this cell, we're firing up our main language model:\n",
    "\n",
    "- 🔧 Model: gpt-3.5-turbo\n",
    "- 🗣️ Verbose mode: On\n",
    "- 🌡️ Temperature: 0.5 (balanced creativity)\n",
    "- 🔑 API Key: Securely loaded\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt= ChatOpenAI(model='gpt-3.5-turbo',\n",
    "                   verbose=True,\n",
    "                   temperature=0.5,\n",
    "                   openai_api_key=os.getenv('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🌐 Defining Our Knowledge Sources\n",
    "\n",
    "In this cell, we're specifying the URLs for our RAG model's knowledge base:\n",
    "\n",
    "- 📚 AI Agents Blog Post\n",
    "- 📘 Langchain RAG Tutorial\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=[\"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "      \"https://python.langchain.com/v0.2/docs/tutorials/rag/\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📥 Fetching and Processing Web Content\n",
    "\n",
    "Here, we're loading and preprocessing our web data:\n",
    "\n",
    "- 🕸️ Using WebBaseLoader to fetch content\n",
    "- 🍲 Employing BeautifulSoup for parsing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = WebBaseLoader(web_paths=(urls),bs_kwargs=dict(parse_only=bs4.SoupStrainer()))\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✂️ Chunking Our Text Data\n",
    "\n",
    "In this cell, we're breaking down our text into manageable pieces:\n",
    "\n",
    "- 📏 Chunk size: 1000 characters\n",
    "- 🔀 Overlap: 200 characters\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🗄️ Creating Our Vector Database\n",
    "\n",
    "Here, we're setting up our FAISS vector database:\n",
    "\n",
    "- 🔢 Creating embeddings with OpenAI\n",
    "- 📚 Storing document chunks for quick retrieval\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore =  FAISS.from_documents(splits, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Setting Up Our Information Retriever\n",
    "\n",
    "In this cell, we're creating a retriever from our vector database:\n",
    "\n",
    "- 🕵️ Ready to fetch relevant information for user queries\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📝 Crafting Our System Prompt\n",
    "\n",
    "Here, we're defining the behavior of our AI assistant:\n",
    "\n",
    "- 🎭 Role: Question-answering assistant\n",
    "- 📚 Context: Use retrieved information\n",
    "- 🚫 Honesty: Admit when answer is unknown\n",
    "- 📏 Conciseness: Max three sentences\n",
    "- 🔧 Output: JSON format\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    "    \"Your response should be a valid JSON object.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧩 Building Our Prompt Template\n",
    "\n",
    "In this cell, we're creating a structured prompt template:\n",
    "\n",
    "- 🤖 System message: Defining AI behavior\n",
    "- 👤 Human input: Placeholder for user queries\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =ChatPromptTemplate.from_messages(\n",
    "   [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ],       \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Defining Our Response Structure\n",
    "\n",
    "Here, we're creating a format for our AI's responses:\n",
    "\n",
    "- ✅ Answer: The main response to the query\n",
    "- 🎵 Poem: A related poetic touch\n",
    "- ❓ Follow-up: Suggested next question\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseFormatter(BaseModel):\n",
    "    answer: str = Field(description=\"The answer to the user's question\")\n",
    "    poem: str = Field(description='poem related to the user query')\n",
    "    followup_question: str = Field(description=\"A followup question the user could ask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏗️ Enabling Structured Outputs\n",
    "\n",
    "In this cell, we're ensuring our LLM provides structured responses:\n",
    "\n",
    "- 🔧 Binding our ResponseFormatter to the LLM\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=chatgpt.with_structured_output(ResponseFormatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📄 Creating Document Formatting Function\n",
    "\n",
    "Here, we're defining a helper function to format retrieved documents:\n",
    "\n",
    "- 🔗 Joining document contents with newlines\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔄 Defining JSON Conversion Function\n",
    "\n",
    "In this cell, we're creating a function to convert responses to JSON:\n",
    "\n",
    "- 📦 Transforming structured output to JSON string\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_json_string(response_formatter: ResponseFormatter) -> str:\n",
    "    return json.dumps(response_formatter.dict(), indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔗 Assembling Our RAG Chain\n",
    "\n",
    "Here, we're putting all the pieces together:\n",
    "\n",
    "1. 🔍 Retrieve relevant documents\n",
    "2. 📝 Apply our prompt template\n",
    "3. 🤖 Process through our LLM\n",
    "4. 🔄 Convert to JSON string\n",
    "5. 📊 Parse the output\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs, \n",
    "        \"input\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | to_json_string\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Testing Our RAG Model\n",
    "\n",
    "In this cell, we're putting our model to the test:\n",
    "\n",
    "- ❓ Query: \"What is an AGENT?\"\n",
    "- 🧠 Generating a comprehensive response\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer:\n",
      "An agent is a virtual character controlled by a Large Language Model (LLM) in the context of Generative Agents Simulation. These agents exhibit human-like behavior and interact with each other in a sandbox environment.\n",
      "\n",
      "\n",
      "poem:\n",
      "An agent in the virtual space, controlled with grace, interacts in a simulated place.\n",
      "\n",
      "\n",
      "followup_question:\n",
      "Would you like to know more about how agents are designed and function in autonomous systems?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query='What is an AGENT?'\n",
    "response=rag_chain.invoke(query)\n",
    "json_response=json.loads(response)\n",
    "\n",
    "for key, value in json_response.items():\n",
    "    print(f'{key}:')\n",
    "    print(value)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎉 Congratulations! You've Built a RAG Model! 🚀\n",
    "\n",
    "---\n",
    "\n",
    "## 🌟 What We've Accomplished\n",
    "\n",
    "In this notebook, we've embarked on an exciting journey through the world of Retrieval-Augmented Generation (RAG). Let's recap our amazing achievements:\n",
    "\n",
    "1. 📚 Imported and organized a powerful set of libraries\n",
    "2. 🧠 Initialized a state-of-the-art language model\n",
    "3. 🌐 Loaded and processed web data to create our knowledge base\n",
    "4. 🗄️ Built a vector database for efficient information retrieval\n",
    "5. 🎨 Crafted an intelligent prompt system\n",
    "6. 🔗 Assembled a comprehensive RAG chain\n",
    "7. 🚀 Tested our model with real-world queries\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "###  Happy coding, and may your AI adventures be ever fruitful! 🌱"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
